---
title: "CITY_US_new"
output:
    html_document:
        # df_print: paged
        # toc: true
        # toc_float: true 
---


```{r, include=FALSE}
knitr::opts_chunk$set(fig.width = 8, fig.height = 8)
# knitr::opts_chunk$set(
#     # This should allow Rmarkdown to locate the data
#     root.dir = rprojroot::find_rstudio_root_file()
# )
library(readxl)
library(reshape2)
library(ggplot2)
library(GGally)
library(stringr)
library(correlation) # cor_sort
library(forcats)
library(tidyr)
library(symmetry)
library(moments)
library(qqplotr)
library(nortest)
library(entropy)
library(car)
library(ppcor)
library(dplyr)
library(lm.beta)
library(olsrr)
library(mvtnorm)
library(MASS)
library(rstatix)
```

# 1 Предварительный анализ данных

```{r}
if (interactive() && !str_ends(getwd(), "R/stat_anal/CITY_US")) {
    setwd("R/stat_anal/CITY_US")
}

data <- read_excel("CITY_shortname.xls")
data[data == "NA"] <- NA
data[, -(1:2)] <- data.frame(lapply(data[, -(1:2)], as.numeric))
fullnames <- names(read_excel("CITY.xls"))
```

## 1.1 Разобраться в том, что означают признаки.

```{r}
print(fullnames)
```

## 1.2 Отобрать признаки

```{r}
names_interesting <- c("AREA", "POP80", "POP92", "POPDEN", "CRIME", "BORN_F", "POVERT", "INCOME", "UNEMP", "TEMPER")

data <- data %>% select(all_of(c("CITY", "STATE", names_interesting)))

print(head(data))
```

## 1.3 Определить вид признаков

Город и штат качественные, остальные количественные, ранги были порядковыми.


```{r}
find_mode_freq <- function(x) {
    x <- x[!is.na(x)]
    return(max(tabulate(match(x, x))))
}

print(data %>% summarise(across(
    all_of(names_interesting),
    find_mode_freq
)))

print(sort(data$UNEMP))
```

Все количественные буду считать непрерывными.
возможно UNEMP непрерывный с плохой точностью.

## 1.4 не актуально

## 1.5 Построить matrix plot

```{r, message=FALSE, warning=FALSE}
# if (interactive()) pdf("ggpairs_unedited.pdf")
# ggpairs(
#     data[, -(1:2)],
#     lower = list(continuous = wrap("points", alpha = 0.5, size = 0.3)),
#     diag = list(continuous = "barDiag")
# )
# if (interactive()) dev.off()
```

## 1.7 outliers

Убираю outliers:

Помечаю некорректные данные в INCOME как NA.
Удаляю город из Аляски за плотность населения.
Флорида выделсется на BORN_F-INCOME
Гаваи выделяются низким уровнем безработицы. странно?

```{r}
data$INCOME[data$INCOME < 100] <- NA
data <- data %>% filter(STATE != "AK")
```

## 1.6 Несимметричные распределения

Функция, которая логарифмирует, если это сделает выборку симметричнее

```{r}
log_asymmetric <- function(x) {
    if (skewness(x, na.rm = TRUE) < abs(skewness(log(x), na.rm = TRUE))) {
        print("default")
        return(x)
    } else {
        print("logged")
        return(log(x))
    }
}
```

Автоматически логарифмирую то что имеет асимметрию  и длинный хвост справа

```{r}
data_logged <- data %>%
    mutate(across(all_of(names_interesting), log_asymmetric))
```

```{r, message=FALSE, warning=FALSE}
# if (interactive()) pdf("ggpairs_logged.pdf")
# ggpairs(
#     data_logged[, -(1:2)],
#     lower = list(continuous = wrap("points", alpha = 0.5, size = 0.3)),
#     diag = list(continuous = "barDiag")
# )
# if (interactive()) dev.off()
```

## 1.8 однородность

выглядит однородно.


## .9 не актуально

## 1.10 всякие характеристики

```{r}
print_characteristics <- function(x) {
    list(
        mean = mean(x, na.rm = TRUE),
        var = var(x, na.rm = TRUE),
        skewness = skewness(x, na.rm = TRUE),
        kurtosis = kurtosis(x, na.rm = TRUE) - 3
    )
}

sapply(data_logged %>% select(all_of
(names_interesting)), print_characteristics)
```

# регрессия

```{r}
df <- data_logged %>%
    select(-CITY, -STATE) %>%
    drop_na()
df_names <- data_logged %>%
    drop_na()

ggplot(melt(cor(df))) +
    geom_raster(aes(x = Var2, y = Var1, fill = value)) +
    geom_text(aes(x = Var2, y = Var1, label = round(value, 2))) +
    scale_fill_gradient2() +
    theme_dark() +
    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```

# 2 Задаете зависимые и 'независимые' переменные (регрессоры), не забываете обратить внимание на выбор способа обработки пропущенных наблюдений, функция lm.

```{r}
model_ <- lm(CRIME ~ ., data = df, na.action = na.exclude)
model <- lm.beta(model_)
summary(model)


ggplot(melt(cov2cor(vcov(model)))) +
    geom_raster(aes(x = Var2, y = Var1, fill = value)) +
    geom_text(aes(x = Var2, y = Var1, label = round(value, 2))) +
    scale_fill_gradient2() +
    theme_dark() +
    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))

# cov2cor(vcov(model))
```

# 3 Интерпретируете результаты регрессии, включая результат lm.beta (в частности, о разнице между b и beta, о значимости и пр.).

```{r}
hist(residuals(model))

shapiro.test(residuals(model))

plot(fitted(model), residuals(model))

abline(h = 0, lty = 2)
```

# 4 Далее есть три проблемы, из-за которых результаты регрессии могут быть неправильными

## линейная модель регрессии не соответствует данным в данных
    
## могут быть сильно зависимые «независимые» переменные
    есть население в 80 и 92 и другие зависимые
## также могут быть outliers.
    outliers уже убраны, если какие-то и остались, это не так критично
    

# 5 Как строятся доверительные интервалы и двумерные доверительные области. 
На примере пары признаков строите двумерный доверительный интервал для пары значащих коэффициентов регрессии, интерпретируете: (1) оба признака влияют на результат согласно оценкам коэффициентов регрессии перед ними: или (2) признаки вместе сильно влияют, но не различить, какой из них больше; или (3) непонятно, или оба признака слабо влияют, или оба влияют сильно. 

```{r}
plot_ellipse <- function(model_, id) {
    # confidenceEllipse(lm.beta(model_), which.coef = id, vcov. = function(x) cov2cor(vcov(x)))
    # points(0, 0, col = "red", pch = 19)
    # lines(x = c(0, coef(lm.beta(model_))[id[1]]), c(0, coef(lm.beta(model_))[id[2]]), col = "red")
    confidenceEllipse(model_, which.coef = id)
    points(0, 0, col = "red", pch = 19)
    lines(x = c(0, coef(model_)[id[1]]), c(0, coef(model_)[id[2]]), col = "red")

    # if (id[1] != 1 && id[2] != 1) {
    #     confidenceEllipse(lm.beta(model_), which.coef = id, vcov. = function(x) cov2cor(vcov(x)))
    #     points(0, 0, col = "red", pch = 19)
    #     lines(x = c(0, coef(lm.beta(model_))[id[1]]), c(0, coef(lm.beta(model_))[id[2]]), col = "red")
    # }
}
```


```{r}
plot_ellipse(model_, c(3, 6))
plot_ellipse(model_, c(3, 7))
# plot_ellipse(model, c(3, 8))
# plot_ellipse(model, c(3, 9))
# plot_ellipse(model, c(3, 10))
# plot_ellipse(model_, c(4, 5))
# plot_ellipse(model_, c(6, 7))
```


# 6 На примере с двумя «независимыми» признаками пишете формулы и показываете, как корреляция между признаками влияет на качество оценок регрессии.

# 7 Нужно избавляться от лишних, избыточных, признаков.
Сделаем это вручную на основе таблицы Redundancy. Там «независимые переменные» сравниваются по двум критериям - независимость от других «независимых» признаков и зависимость от зависимой переменной. Объясняете, что означают столбцы, что делать, если эти критерии дают противоречивые рекомендации, решаете, какой признак лучше убрать первым.


```{r}
# ols_vif_tol(model_)
```
<!-- 3 маленьких значения у AREA, POP92, POPDEN, потому что они зависят друг от друга -->

```{r}
# ols_correlations(model_)
```

# 8 Убираете вручную на основе Redundancy несколько признаков и смотрите, что меняется (R, adjusted R, значимость регрессии, значимость коэффициентов регрессии, независимость «независимых» переменных, AIC).

```{r}
model_manual <- lm(CRIME ~ ., data = df, na.action = na.exclude)
summary(model_manual)
ols_vif_tol(model_manual)
ols_correlations(model_manual)
AIC(model_manual)
```

Удаляю AREA из-за VIF и partial

```{r}
model_manual <- lm(CRIME ~ POP80 + POP92 + POPDEN + INCOME + BORN_F + POVERT + UNEMP + TEMPER, data = df, na.action = na.exclude)
summary(model_manual)
ols_vif_tol(model_manual)
ols_correlations(model_manual)
AIC(model_manual)
```

Удаляю POP80 из-за VIF

```{r}
model_manual <- lm(CRIME ~ POP92 + POPDEN + INCOME + BORN_F + POVERT + UNEMP + TEMPER, data = df, na.action = na.exclude)
summary(model_manual)
ols_vif_tol(model_manual)
ols_correlations(model_manual)
AIC(model_manual)
```

Удаляю INCOME из-за VIF и Partial

```{r}
model_manual <- lm(CRIME ~ POP92 + POPDEN + BORN_F + POVERT + UNEMP + TEMPER, data = df, na.action = na.exclude)
summary(model_manual)
ols_vif_tol(model_manual)
ols_correlations(model_manual)
AIC(model_manual)
```

Удаляю POP92 из-за Partial

```{r}
model_manual <- lm(CRIME ~ POPDEN + BORN_F + POVERT + UNEMP + TEMPER, data = df, na.action = na.exclude)
summary(model_manual)
ols_vif_tol(model_manual)
ols_correlations(model_manual)
AIC(model_manual)
```

Удаляю TEMPER из-за Partial

```{r}
model_manual <- lm(CRIME ~ POPDEN + BORN_F + POVERT + UNEMP, data = df, na.action = na.exclude)
summary(model_manual)
ols_vif_tol(model_manual)
ols_correlations(model_manual)
AIC(model_manual)
```

Удаляю UNEMP из-за Partial

```{r}
model_manual <- lm(CRIME ~ POPDEN + BORN_F + POVERT, data = df, na.action = na.exclude)
summary(model_manual)
ols_vif_tol(model_manual)
ols_correlations(model_manual)
AIC(model_manual)
```

все

# 9 Далее переходите к автоматической пошаговой регрессии по AIC. По результатам определяете, сколько признаков оставить. Сравниваете результаты Forward и Backward вариантов.

```{r}
model.start <- lm(CRIME ~ 1, data = df, na.action = na.exclude)
model.end <- model_

stepAIC(model.start, direction = "forward", scope = list(lower = model.start, upper = model.end))
stepAIC(model_, direction = "backward")
stepAIC(model_manual, direction = "both", scope = list(lower = model.start, upper = model.end))

modelAIC_ <- lm(CRIME ~ AREA + POP80 + POPDEN + BORN_F + POVERT + TEMPER, data = df, na.action = na.exclude)
modelAIC <- lm.beta(modelAIC_)

summary(modelAIC)

AIC(modelAIC)
```

```{r}
plot_ellipse(model_manual, c(1, 2))
plot_ellipse(model_manual, c(1, 3))
plot_ellipse(model_manual, c(1, 4))
plot_ellipse(model_manual, c(2, 3))
plot_ellipse(model_manual, c(2, 4))
plot_ellipse(model_manual, c(3, 4))



plot_ellipse(modelAIC_, c(2, 3))
plot_ellipse(modelAIC_, c(2, 4))
plot_ellipse(modelAIC_, c(2, 7))
plot_ellipse(modelAIC_, c(3, 4))
plot_ellipse(modelAIC_, c(3, 7))
plot_ellipse(modelAIC_, c(4, 5))

# plot_ellipse(modelAIC_, c(1, 2))
# plot_ellipse(modelAIC_, c(1, 3))
# plot_ellipse(modelAIC_, c(1, 4))
# plot_ellipse(modelAIC_, c(1, 5))
# plot_ellipse(modelAIC_, c(1, 6))
# plot_ellipse(modelAIC_, c(1, 7))
# plot_ellipse(modelAIC_, c(2, 5))
# plot_ellipse(modelAIC_, c(2, 6))
# plot_ellipse(modelAIC_, c(3, 5))
# plot_ellipse(modelAIC_, c(3, 6))
# plot_ellipse(modelAIC_, c(4, 6))
# plot_ellipse(modelAIC_, c(4, 7))
# plot_ellipse(modelAIC_, c(5, 6))
# plot_ellipse(modelAIC_, c(5, 7))
# plot_ellipse(modelAIC_, c(6, 7))
```

# 10 Строите обычную регрессию по выбранному числу признаков. Сначала смотрите на нормальность остатков (зачем нужно на это смотреть?), затем смотрите на график Predicted vs Residuals. Как по нему понять, адекватна ли линейная регрессия? Как будет выглядеть график, если на самом деле была квадратичная зависимость (в случае одной независимой переменной)? Как может повлиять на этот график выбор Pairwise deletion для пропущенных наблюдений?

```{r}
# hist(residuals(modelAIC_))
# shapiro.test(residuals(modelAIC_))

# ols_plot_resid_stud_fit(modelAIC_)

# # print(df_names[c(15, 27, 38, 56, 52), ])

# ols_plot_resid_stud(modelAIC_)
# # print(df_names[56, ])



hist(residuals(model_manual))
shapiro.test(residuals(model_manual))

ols_plot_resid_stud_fit(model_manual)

# print(df_names[c(15, 27, 38, 56, 52), ])

ols_plot_resid_stud(model_manual)
# print(df_names[56, ])

model_manual_ <- model_manual
model_manual <- lm.beta(model_manual)
```

# 11 Далее переходите к поиску outliers. Сначала смотрите на скаттерплот Residuls vs Deleted Residuals. Нужно объяснить, что этот такое, что откладывается по осям, как там найти outliers.

```{r}
# ggplot(data_frame(residuals = rstandard(modelAIC), studres = studres(modelAIC)), aes(x = residuals, y = studres)) +
#     geom_point() +
#     geom_abline(slope = 1, intercept = 0, color = "red")

ggplot(data_frame(residuals = rstandard(model_manual), studres = studres(model_manual)), aes(x = residuals, y = studres)) +
    geom_point() +
    geom_abline(slope = 1, intercept = 0, color = "red")

# ggplot(data_frame(residuals = rstandard(modelAIC), studres = studres(modelAIC)), aes(x = residuals, y = studres - residuals)) +
#     geom_point() +
#     geom_abline(slope = 0, intercept = 0, color = "red")
```



# 12 Затем смотрите на разные меры, например, outliers по Куку и по Махаланобису (в пространстве регрессоров). Объясняете, что это такое, по отношению к чему это outliers. Умеете приводить пример, где, в случае одной независимой переменной, находится outliers по Куку, но не по Махаланобису, и наоборот.

```{r}
plot(sort(mahalanobis_distance(df)$mahal.dist, decreasing = TRUE))
# plot(cooks.distance(modelAIC))

# df_names[which(cooks.distance(modelAIC) > 0.06), ]


plot(sort(cooks.distance(model_manual), decreasing = TRUE))
df_names[which(cooks.distance(model_manual) > 0.1), ]
```


# 13 Итог: результат линейной регрессии, для которой проверена адекватность модели, значимость, отсутствие outliers, проинтерпретированы коэффициенты регрессии. Спрогнозируйте что-нибудь по построенной регрессионной модели, посмотрите на доверительные и предсказательные интервалы.


```{r}
new <- data_logged %>%
    filter(CITY == "EL_PASO") %>%
    select(POPDEN, BORN_F, POVERT)



predict.lm(model_manual_, new)
```

.......................
