\documentclass[specialist, substylefile = spbureport.rtx,
    subf,href,colorlinks=true, 12pt]{disser}

% \usepackage[a4paper, mag=1000, includefoot,
%     left=2cm, right=1.5cm, top=2cm, bottom=2cm, headsep=1cm, footskip=1cm]{geometry}

\usepackage[a4paper, top=2cm, bottom=2cm, left=2cm, right=2cm]{geometry}

\usepackage[T1,T2A]{fontenc}

\usepackage{graphicx}
\graphicspath{ {images/} }
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm} %for \newtheorem*
\usepackage[english,russian]{babel}

% \pagestyle{plain}

\newtheorem*{definition}{Определение}
\newtheorem*{example}{Пример}
\newtheorem*{hypothesis}{Гипотеза}
\newtheorem*{question}{Вопрос}
\newtheorem*{algorithm}{Алгоритм}

\newcommand{\rank}{\mathsf{rank}\ }
\newcommand{\Lrank}{\mathsf{rank}_L\ }
\newcommand{\T}{\mathcal{T}}
\newcommand{\F}{\mathsf{F}}
\newcommand{\MF}{\vec{\F}}
\newcommand{\sfS}{\mathsf{S}}
\newcommand{\sfR}{\mathsf{R}}
\newcommand{\MS}{\vec{\sfS}}
\newcommand{\MSE}{\mathsf{MSE}}
\newcommand{\SSA}{\mathsf{SSA}}
\newcommand{\MSSA}{\mathsf{MSSA}}
\newcommand{\ProjSSA}{\mathsf{ProjSSA}}
\newcommand{\mean}{\mathsf{mean}}
\newcommand{\X}{\mathbf{X}}
\newcommand{\wX}{\overset{\wedge}{\X}}






\institution{Санкт-Петербургский государственный университет\\
    Математико-механический факультет\\
    Кафедра Статистического Моделирования
}
\title{«Научно-исследовательская работа» (семестр 6)}
\topic{Исследование условий для поддерживающих временных рядов в MSSA}
\author{Ткаченко Егор Андреевич}
\group{группа 19.Б04-мм}
\sa       {Голяндина Нина Эдуардовна}
\sastatus {к.\,ф.-м.\,н., доцент}
\city{Санкт-Петербург}
\date{2022}


\begin{document}

    \maketitle
    \pagebreak
    \tableofcontents
    \pagebreak

    \intro
        Полезность умения строить прогнозы не нуждается в доказательстве. Прогноз временных рядов может использоваться в прогнозе погоды, приливов, спроса на товары и многом другом.
         
        С помощью книги \cite{SSA_with_R} был изучен базовый $\SSA$, разложение рядов, заполнение пропусков в данных, прогноз и базовый $\MSSA$. Для работы с временными рядами и их прогнозом использовался пакет Rssa. Проведены эксперименты с простейшими моделями сигналов для изучения связи между согласованностью сигналов и поддерживающими рядами. 
        Исследовано, при каких отклонениях, сигналы с одинаковой структурой перестают быть согласованными. Проведено сравнение линейных рядов и их аппроксимаций экспонентой как поддерживающих.

    
        % После этого я изучал 
    % \chapter{обозначения}
    \section{Определения}
        Вещественным временным рядом длины $N$ называется вектор
        $$\F = (f_1, \dots, f_{N}),\ f_j \in \mathbb{R}.$$

        Многомерным временным рядом $\MF$ называется набор $s$ временных рядов $\F^{(p)}$ длин $N_p$:
        $$\MF = \{\F^{(p)} = (f^{(p)}_1, \dots, f^{(p)}_{N_p}),\ p=1, \dots, s\}.$$

        $L$-траекторная матрица (или просто траекторная матрица) ряда $\F$ имеет структуру ганкелевой матрицы, а ее столбцами являются отрезки длины L ряда $\F$
        $$\T_{\SSA}(\F) =
        \begin{pmatrix}
            f_1     & f_2    & \dots  & f_{K} \\
            f_2     & f_3    & \dots  & f_{K+1}     \\
            \vdots  & \vdots & \ddots & \vdots  \\
            f_{L} & f_{L+1}    & \dots  & f_{N} \\
        \end{pmatrix}.$$
        $L$-траекторная матрица многомерного ряда $\MF$ состоит из горизонтально склеенных траекторных матриц рядов $\F^{(p)} \in \MF$:
        $$\T_{\MSSA}(\MF) = [\T_{\SSA}(\F^{(1)}): \dotso :\T_{\SSA}(\F^{(s)})].$$
        Из траекторной матрицы можно восстановить ряд. Из любой матрицы подходящего размера можно получить траекторную матрицу проектированием на пространство ганкелевых матриц (или склеенных горизонтально ганкелевых матриц для многомерного случая).

        $L$-Ранг ряда равен рангу его $L$-траекторной матрицы:
        $$\Lrank \F = \rank \T_{\SSA}(\F),\qquad \Lrank \MF = \rank \T_{\MSSA}(\MF).$$

        Ряд называется рядом конечного ранга $r$, если его $L$-ранг равен $r$ для любой длины окна $L$ и любой достаточно большой длины $N$\cite[раздел 2.1.2.1, стр. 34]{SSA_with_R}.
        


    \section{Применение SSA и MSSA}

        Алгоритмы $\SSA$ и $\MSSA$ могут быть применены для аппроксимации временного ряда рядом конечного ранга.

        \begin{algorithm}\ \\
            \textbf{Вход:} Ряд $\F$ для $\SSA$ или многомерный ряд $\MF$ для $\MSSA$,
            длина окна $L \leq N$ для $\SSA$ или $L \leq N_p, \forall N_p$ для $\MSSA$,
            ранг аппроксимирующего ряда r.

            \begin{enumerate}
                \item[1] Вложение. Временной ряд переводится в L-траекторную матрицу $\X$
                    $$\X = \T_{\SSA}(\F) \text{ для $\SSA$,\qquad } \X = \T_{\MSSA}(\MF) \text{ для $\MSSA$.}$$
                \item[2] Сингулярное разложение. Методом SVD матрица $\X$ раскладывается на сумму $d$ матриц 
                $\X_i = \sqrt{\lambda_i}U_iV_i^T$, где $d = \rank \X = \rank \X\X^T \leq L$,
                $\lambda_i$ --- собственные числа матрицы $\X\X^T$ ($\lambda_1 \geq \dotso \geq \lambda_L \geq 0$),
                $U_i$ --- собственные вектора матрицы $\X\X^T$,
                $V_i = \X^T U_i / \sqrt{\lambda_i}$ --- факторные вектора матрицы $\X$.
                \item[3] Группировка. Множество индексов $\{1, \dots, d\}$ делится на $m$ непересекающихся множеств $I_1 ,\dots, I_m$. Далее для каждого множества индексов (пусть $I = {i_1, \dots, i_t}$) получается матрица $\X_I = \X_{i_1} + \dots + \X_{i_t}$.\\
                Для аппроксимации рядом конечного ранга $r$, понадобится множество из первых $r$ индексов $\{1, \dots, r\}$, соответствующую ему матрицу обозначу $\wX_r = \X_1 + \dots + \X_r$.
                \item[4] 
                Сгруппированные матрицы $\X_{I_j}$ диагональным усреднением восстанавливаются в ряды ($\SSA$) или многомерные ряды ($\MSSA$).
                Для получения аппроксимирующего ряда нужно восстановить его из матрицы $\wX_r$.
            \end{enumerate}
            \textbf{Выход:} Аппроксимирующий ряд $\overset{\wedge}{\F}_r$ конечного ранга r.
        \end{algorithm}

    \section{ЛРФ}
        Линейная рекуррентная формула (ЛРФ) выражает каждый член последовательности через линейную комбинацию предыдущих членов.

        Ряд $\F$ длины $N$ --- управляемый ЛРФ, если существуют такие $a_1, \dotso, a_d$, что:
        $$f_{i+d} = \sum_{k=1}^d a_k f_{i+d-k},\ 1 \leq i \leq N - d,\ a_d \neq 0,\ d < N - 1.$$
        Важно отметить, что бесконечный ряд конечного ранга является управляемым ЛРФ \cite[раздел 2.1.2.2, стр. 35]{SSA_with_R}.
        
        

        Вещественный временной ряд $\F$, управляемый ЛРФ, естественным образом прогнозируется на одну точку:
        $$\overset{\sim}{f}_{N} = \sum_{k=1}^{L-1} a_k f_{N-k}.$$
        Но тогда его можно прогнозировать и на любое количество точек.
        % $$\overset{\sim}{\F}_{N_p} = (\overset{\sim}{f}_{N_p}, \dots, \overset{\sim}{f}_{N_p + \overset{\sim}{N}_p - 1}), \overset{\sim}{f_j} \in \mathbb{R}.$$
        % Также ряд управляемый ЛРФ, естественным образом можно прогнозировать на одно значение, а значит и на любое количество значений.

        % \begin{block}{Коэффициенты ЛРФ $a_1, \dotso, a_{L-1}$}
        %     $$(a_1, \dotso, a_{L-1}) = \mathcal{R}_L=\frac{1}{1-\sum_{j=1}^r \pi(U_j)^2} \sum_{j=1}^r \pi(U_j) \underline{U_j},$$
        %     где $\pi(U_j)$ --- последняя координата вектора $U_j$,\\ $\underline{U_j}$ --- вектор $U_j$ без последней координаты.
        % \end{block}



    \chapter{Постановка задачи}

        Пусть имеется временной ряд $\F^{(1)} = \sfS^{(1)} + \sfR^{(1)}$, где сигнал $\sfS^{(1)}$ --- ряд управляемый ЛРФ, шум~$\sfR^{(1)}$ --- ряд без структуры. Рассмотрим задачу прогнозирования $\sfS^{(1)}$. Эта задача уже решается методом $\SSA$, но как можно улучшить прогноз?

        Пусть помимо ряда $\F^{(1)}$ имеется временной ряд $\F^{(2)} = \sfS^{(2)} + \sfR^{(2)}$.
        Если структура сигналов $\sfS^{(1)}$ и $\sfS^{(2)}$ похожа, то использование ряда $\F^{(2)}$ может улучшить прогноз сигнала $\sfS^{(1)}$, потому что второй ряд дает алгоритму больше данных, которые могут улучшить ЛРФ.
        Возможность такого улучшения прогноза подтверждена \cite[4.3.3.3, стр. 216]{SSA_with_R}.
        Но второй ряд может сделать прогноз хуже, если структура его сигнала отличается от первого или шум большой.

        Простейший пример похожих по структуре сигналов --- гармонические колебания с одинаковыми периодом. Они даже могут быть смещены по фазе или иметь разную амплитуду.

        Для объективной оценки качества прогноза будем использовать среднюю квадратичную ошибку.
        $$\mathsf{MSE(\overset{\sim}{S},\ S)} = \frac{1}{N_{f}} \sum_{i = N+1}^{N + N_{f}} (\overset{\sim}{s}_i - s_i)^2,$$
        где $\overset{\sim}{\sfS}$ --- прогноз сигнала $\sfS$ на $N_{f}$ точек, $N$ --- длина прогнозируемого сигнала $\sfS$.
    

        Ряд $F^{(2)}$ называется поддерживающим для прогноза, если прогноз с его использованием лучше чем без него, т.е.
    
        $$\MSE(\overset{\sim}{\sfS}_{\MSSA},\ \sfS^{(1)}) < \MSE(\overset{\sim}{\sfS}_{\SSA},\ \sfS^{(1)}).$$
        Такое определение можно применить только в экспериментах с известным продолжением ряда. На практике не с чем сравнить прогноз, поэтому появляется вопрос. Как понять, что ряд поддерживающий не зная продолжения прогнозируемого ряда? Помочь ответить на этот вопрос может понятие согласованности.

        Сигналы $\sfS^{(1)}$, $\sfS^{(2)}$ называются полностью согласованными, если ранг $r_{1,2} = r_1 = r_2$ и полностью несогласованными, если $r_{1,2} = r_1 + r_2$, где $r_1 = \rank \sfS^{(1)}$, $r_2 = \rank \sfS^{(2)}$, $r_{1,2} = \rank \MS = \rank \{\sfS^{(1)}, \sfS^{(2)}\}.$

    % \section{Примеры}
    %     когда следует ожидать, что $\MSSA$ лучше
        % \begin{example}

        %     \begin{tabular}{lll}
        %     & $s^{(i)}_j = A_i\cos(\frac{2\pi j}{T_i})$ & $s^{(i)}_j = A_i\exp(j\lambda_i)$ \\
        %     Согласованные:  & $T_1 = T_2 \neq 2$ & $\lambda_1 = \lambda_2$
        %     \\
        %     Несогласованные:& $2 \neq T_1 \neq T_2 \neq 2$ & $\lambda_1 \neq \lambda_2$
        %     \end{tabular}
        % \end{example}

    \chapter{Численные эксперименты}
        
    \section{Первый эксперимент: выбор компонент для MSSA}
        По определению, когда сигналы похожи, их можно считать согласованными и лучше использовать (при прогнозе или восстановлении сигнала) ранг равный рангу одного сигнала. Когда сигналы отличаются, их следует считать не согласованными и использовать ранг равный сумме рангов сигналов. Исследуем, будет ли ошибка $\MSSA$ меньше при таком выборе ранга для алгоритма $\MSSA$ в восстановлении и прогнозе первого ряда. И будут ли при этом вторые ряды поддерживающими.

        Выберем в качестве первого ряда простой сигнал, зависящий от параметра (например, у косинуса параметр --- период) с аддитивным гауссовым шумом с дисперсией $\sigma_1^2 = 0.2^2$.
        Второй ряд будет простым сигналом того же вида, с несколько отличающимся параметром и без шума.
        Восстановим и спрогнозируем первый ряд с помощью $\SSA$, $\MSSA$ считая ряды согласованными и $\MSSA$ считая ряды несогласованными.

        Для устойчивости результатов, повторим это 50 раз и усредним ошибки.

        Назовем относительной ошибкой прогноза (восстановления) значение $$\displaystyle \frac{error_{\SSA} - error_{\MSSA}}{error_{\SSA} + error_{\MSSA}},$$ где $error_{\SSA}, error_{\MSSA}$ --- ошибки прогноза (восстановления) методами $\SSA$ и $\MSSA$ соответственно.
        
        Значения относительной ошибки легко расположить на графике (она принимает значения от -1 до 1). По значению относительной ошибки легче понять, какой метод лучше (не надо сравнивать два значения ошибок, которые просто положительны и могут быть любых порядков). Но относительную ошибку нельзя считать когда $error_{\SSA} = error_{\MSSA} = 0$.
        
        Как интерпретировать значения относительной ошибки? 
        \begin{itemize}
            \item значения больше $0$ значат, что что $\MSSA$ лучше $\SSA$;
            \item значения меньше $0$ значат, что что $\MSSA$ хуже $\SSA$;
            \item значения около $0$ значат что ошибки примерно равны;
            \item значения далеко от $0$ значат, что ошибки сильно отличаются.
        \end{itemize}

        Все сигналы в этом и следующих экспериментах будут нормироваться, чтобы амплитуда сигнала не влияла на ошибки прогноза и восстановления. Например, для косинуса: $s_j^{(i)} = A \cos(\frac{2\pi j}{T_i})$, где $A$ --- такая константа, что $\mean(|s_j^{(i)}|) = 1$.


    \subsection{Сигнал косинус}
        Функция для сигналов --- $s^{(i)}_j = A \cos(\frac{2\pi j}{T_i})$.
        Сигнал $\sfS^{(1)}$ --- косинус с периодом $T_1 = 8$.
        Сигналы $\sfS^{(2)}$ --- косинусы с периодами $T_2 \in \{8$, $8.02$, $8.04$, $8.06$, $8.08$, $8.1$, $8.15\}$.

        Ранг косинуса равен 2, поэтому для $\MSSA$ используются первые 2 или первые 4 компоненты разложения, а для $\SSA$ только 2.

        \begin{figure}[h]
            \centering
            \includegraphics[height=11cm, width=0.8\textwidth]{experiment_1_cos.pdf}
            \caption{Зависимость относительных ошибок от разницы сигналов и выбора ранга для $\MSSA$.}
            \label{fig:exp1_cos}
        \end{figure}

        На рис. \ref{fig:exp1_cos} видим, что с увеличением разницы периодов рядов использование четырех компонент становится лучше и для прогноза и для восстановления сигнала, но при этом второй ряд является поддерживающим только для случаев, когда второй сигнал совпадает с первым или очень близок к нему, а ранг для $\MSSA$ 2.

    \subsection{Сигнал экспонента (показательная функция)}
        Функция для сигналов --- $s^{(i)}_j = A \exp(j\lambda_i)$.
        Сигнал $\sfS^{(1)}$ --- нормированная показательная функция c $\lambda_1 = 0.005$.
        Сигналы $\sfS^{(2)}$ --- нормированная показательная функция c $\lambda_2 \in \{0.005$, $0.0075$, $0.01$, $0.0125$, $0.015$, $0.02$, $0.025$, $0.03\}$.

        Ранг показательной функции равен 1, поэтому для $\MSSA$ используется первая или первые 2 компоненты разложения, а для $\SSA$ только первая.

        \begin{figure}[h]
            \centering
            \includegraphics[height=11cm, width=0.8\textwidth]{experiment_1_exp.pdf}
            \caption{Зависимость относительных ошибок от разницы сигналов и выбора ранга для $\MSSA$.}
            \label{fig:exp1_exp}
        \end{figure}

        На рис. \ref{fig:exp1_exp} видим похожий результат: с отдалением $\lambda_2$ от $\lambda_1$ использование двух компонент становится лучше. Второй ряд поддерживающий только для случаев, когда он равен первому или очень близок к нему, но на этот раз не только когда ранг для алгоритма $\MSSA$ равен рангу ряда.

    \subsection{Сигнал косинус с показательной модуляцией (общий период, меняющаяся модуляция)}
        Функция для сигналов --- $s^{(i)}_j = A \exp(j\lambda_i) \cos(\frac{2\pi j}{8})$.
        Сигнал $\sfS^{(1)}$ --- функция c $\lambda_1 = 0.005$.
        Сигналы $\sfS^{(2)}$ --- функция c $\lambda_2 \in \{0.005$, $0.0075$, $0.01$, $0.0125$, $0.015$, $0.02$, $0.025$, $0.03\}$.

        Ранг косинуса с модуляцией равен 2, поэтому для $\MSSA$ используются первые 2 или первые 4 компоненты разложения, а для $\SSA$ только 2.

        \begin{figure}[h]
            \centering
            \includegraphics[height=11cm, width=0.8\textwidth]{experiment_1_expcos1.pdf}
            \caption{Зависимость относительных ошибок от разницы сигналов и выбора ранга для $\MSSA$.}
            \label{fig:exp1_expcos1}
        \end{figure}

        На рис. \ref{fig:exp1_expcos1} видим аналогичный результат для косинусов с модуляцией при изменении модулирующей функции.

    \subsection{Сигнал косинус с показательной модуляцией (меняющийся период, общая модуляция)}
        Функция для сигналов --- $s^{(i)}_j = A \exp(0.02j) \cos(\frac{2\pi j}{T_i})$.
        Сигнал $\sfS^{(1)}$ --- функция c $T_1 = 8$.
        Сигналы $\sfS^{(2)}$ --- функция c $T_2 \in \{8$, $8.02$, $8.04$, $8.06$, $8.08$, $8.1$, $8.15\}$.

        Ранг косинуса с модуляцией равен 2, поэтому для $\MSSA$ используются первые 2 или первые 4 компоненты разложения, а для $\SSA$ только 2.

        \begin{figure}[h]
            \centering
            \includegraphics[height=11cm, width=0.8\textwidth]{experiment_1_expcos2.pdf}
            \caption{Зависимость относительных ошибок от разницы сигналов и выбора ранга для $\MSSA$.}
            \label{fig:exp1_expcos2}
        \end{figure}

        На рис. \ref{fig:exp1_expcos2} видим аналогичный результат для косинусов с модуляцией при изменении периодов.

    \subsection{Результат первого эксперимента}

        Для всех видов сигналов при отклонении второго сигнала от первого всегда наступал момент, когда использование удвоенного ранга дает меньшие ошибки прогноза и восстановления.

        Но при этом, второй ряд редко оказывался поддерживающим, потому что большая часть наблюдений находилась в нижней левой четверти.

        % Так как большая часть наблюдений оказалась в нижней левой четверти графика относительных ошибок, это значит что во всех случаях, кроме $\MSSA$ с маленькой разницей сигналов и рангом равным рангу сигнала, использование $\SSA$ дает лучший результат.

    \section{Второй эксперимент: сравнение ошибки прогноза методами SSA и MSSA при разных величинах шума первого ряда и отклонениях второго ряда}
        Никита Федоров в свей выпускной квалификационной работе изучал влияние величины второго шума на результаты работы $\SSA, \MSSA, \ProjSSA$ \cite[глава 3, стр. 17]{supportive_mssa}. Рассмотрим влияние величины первого шума на прогноз $\SSA$ и $\MSSA$, с не зашумленным вторым рядом.

        Гипотеза: при увеличении шума первого ряда, $\MSSA$ станет лучше для любого отклонения второго ряда. Если это так, то можно найти зависимость граничного значения $\sigma_1$ (при котором $\SSA$ становится хуже $\MSSA$) от изменения параметра второго сигнала.

        Как и в первом эксперименте, выберем в качестве первого ряда простой сигнал, зависящий от параметра с аддитивным гауссовым шумом с несколькими значениями дисперсией $\sigma_1^2$.
        Второй ряд будет простым сигналом того же вида, с несколько отличающимся параметром и без шума.
        Спрогнозируем первый ряд с помощью $\SSA$ и $\MSSA$ используя в алгоритме ранг равный рангу сигнала.
        Для устойчивости результатов, повторим это 50 раз и найдем средние ошибки.
        Если графики ошибок прогноза $\SSA$ и $\MSSA$ будут пересекаться, то найдем значения $\sigma_1$ при которых это происходит, это и будут граничные значения $\sigma_1$.

    \subsection{Сигнал косинус}
        Модель сигнала --- $s_j^{(i)} = A \cos(\frac{2\pi j}{T_i})$. Параметры для сигналов: $T_1 = 8$, $T_2 \in \{8$, $8.02$, $8.04$, $8.06$, $8.08$, $8.1$, $8.15\}$, $\sigma_1 \in \{0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 1\}$, ранг косинуса равен 2.


        \begin{figure}[h]
            \centering
            \begin{minipage}{.45\textwidth}
                \centering
                \includegraphics[width=\textwidth]{experiment_2_cos1.pdf}
                \caption{Ошибка прогноза $\SSA$ и $\MSSA$ для косинусов.}
                \label{fig:exp2_cos1}
            \end{minipage}\hfill
            \begin{minipage}{.45\textwidth}
                \centering
            \includegraphics[width=\textwidth]{experiment_2_cos2.pdf}
            \caption{Зависимость граничного значения $\sigma_1$ от периода второго сигнала}
            \label{fig:exp2_cos2}
            \end{minipage}
        \end{figure}

        На рис. \ref{fig:exp2_cos1} видим, что график ошибки прогноза $\SSA$ (черная линия) пересекает все графики ошибок прогноза $\MSSA$ кроме одного, но они очевидно пересекутся при большем $\sigma_1$. Пересечения графиков будем искать с помощью интерполяции, а для случаев, когда пересечения не было --- с помощью экстраполяции. 

        На рис. \ref{fig:exp2_cos2} изображены полученные граничные значения $\sigma_1$ для каждого второго сигнала. Видна линейная зависимость.

    \subsection{Сигнал экспонента (показательная функция)}
        Модель сигнала --- $s^{(i)}_j = A \exp(j\lambda_i)$. Параметры для сигналов: $\lambda_1 = 0.005$, $\lambda_2 \in \{0.005$, $0.01$, $0.012$, $0.013$, $0.015$, $0.0175$, $0.02\}$, $\sigma_1 \in \{0, 0.3, 0.6, 1, 1.5, 2\}$, ранг косинуса равен 2.


        \begin{figure}[h]
            \centering
            \begin{minipage}{.45\textwidth}
                \centering
                \includegraphics[width=\textwidth]{experiment_2_exp1.pdf}
                \caption{Ошибка прогноза $\SSA$ и $\MSSA$ для показательных функций.}
                \label{fig:exp2_exp1}
            \end{minipage}\hfill
            \begin{minipage}{.45\textwidth}
                \centering
            \includegraphics[width=\textwidth]{experiment_2_exp2.pdf}
            \caption{Зависимость граничного значения $\sigma_1$ от параметра $\lambda_2$ второго сигнала}
            \label{fig:exp2_exp2}
            \end{minipage}
        \end{figure}

        На рис. \ref{fig:exp2_exp1} видим, что график ошибки прогноза $\SSA$ (черная линия) пересекает все графики ошибок прогноза $\MSSA$. 

        На рис. \ref{fig:exp2_exp2} видна линейная зависимость, как с косинусом.
    
    \subsection{Результат второго эксперимента}
        Гипотеза подтверждена, зависимость граничных значений $\sigma_1$ от отклонения второго сигнала линейная или очень близка к линейной.

    \section{Третий эксперимент: линейные сигналы}

        \begin{figure}[h]
            \centering
            \includegraphics[width=0.8\textwidth]{experiment_3_expline.pdf}
            \caption{Пример аппроксимации линейных функций показательными.}
            \label{fig:exp3_expline}
        \end{figure}

        Как видно на рис. \ref{fig:exp3_expline} иногда линейный сигнал можно хорошо аппроксимировать показательной функцией. Причем, качество приближения зависит от угла наклона. Из-за того, что ранг линейного сигнала равен 2, а показательного --- 1, становится интересно, можно ли использовать экспоненциальный сигнал как поддерживающий для линейного?

    \subsection{Является ли первая компонента разложения линейного ряда показательной функцией?}
        Для того чтобы понять, можно ли использовать экспоненциальный сигнал как поддерживающий для линейного, нужно узнать, насколько их структура похожа. Например, сравнить аппроксимацию линейного сигнала рядом ранга 1 (восстановить первую компоненту алгоритмом $\SSA$) и аппроксимацию этого же сигнала экспоненциальной функцией.

        \begin{figure}[h]
            \centering
            \includegraphics[width=0.8\textwidth]{experiment_3_linefirstcomp.pdf}
            \caption{Сравнение первых компонент сигнала и аппроксимирующих экспонент.}
            \label{fig:exp3_linefirstcomp}
        \end{figure}

        Из рис. \ref{fig:exp3_linefirstcomp} видно что первая компонента разложения линейного ряда не является показательной функцией, так как показательные функции не могут дважды пересекаться.

    \subsection{Зависимость доли второй компоненты от угла наклона}

        Как уже было замечено, при меньших углах наклона, аппроксимация получается лучше. Есть ли зависимость доли второй компоненты в линейном сигнале от угла наклона?

        
        \begin{figure}[h]
            \centering
            \begin{minipage}{.45\textwidth}
                \centering
                \includegraphics[width=\textwidth]{experiment_3_secondpart1.pdf}
                \caption{Зависимость доли второй компоненты от угла наклона линейного сигнала.}
                \label{fig:exp3_secondpart1}
            \end{minipage}\hfill
            \begin{minipage}{.45\textwidth}
                \centering
            \includegraphics[width=\textwidth]{experiment_3_secondpart2.pdf}
            \caption{Зависимость логарифма доли второй компоненты от угла наклона линейного сигнала.}
            \label{fig:exp3_secondpart2}
            \end{minipage}
        \end{figure}

        


        % \begin{figure}[h]
        %     \centering
        %     \includegraphics[width=0.8\textwidth]{experiment_3_secondpart.pdf}
        %     \caption{Зависимость доли второй компоненты от угла наклона линейного сигнала.}
        %     \label{fig:exp3_secondpart}
        % \end{figure}

        На рис. \ref{fig:exp3_secondpart1} зависимость похожа на экспоненциальную, прологарифмируем и проверим это.
        На рис. \ref{fig:exp3_secondpart2} видна линейная зависимость логарифма доли второй компоненты и угла наклона сигнала, поэтому доля второй компоненты действительно зависит экспоненциально от угла наклона.

    \subsection{При каком шуме вторая компонента теряется}
        На рис. \ref{fig:exp3_secondpart1} можно заметить, что доля второй компоненты мала, а значит в алгоритме $\SSA$ она может оказаться не второй и быть потеряна при достаточно большом шуме.

        Как понять, что вторая компонента потерялась не изучая разложение в ручную?
        
        Будем восстанавливать одну или две компоненты алгоритмом $\SSA$ из линейных сигналов с разными углами наклона и считать ошибку восстановления. Когда вторая компонента теряется, ошибка двух компонент становится больше ошибки одной компоненты, потому что вместо нужной второй компоненты ряда берется часть шума.
        
        \begin{figure}[h]
            \centering
            \includegraphics[width=0.8\textwidth]{experiment_3_lost1.pdf}
            \caption{Зависимость ошибки восстановления от $\sigma^2$.}
            \label{fig:exp3_lost1}
        \end{figure}

        \begin{figure}[h]
            \centering
            \includegraphics[width=0.8\textwidth]{experiment_3_lost2.pdf}
            \caption{Зависимость ошибки восстановления от $\sigma^2$, крупнее.}
            \label{fig:exp3_lost2}
        \end{figure}

        На рис. \ref{fig:exp3_lost1} и \ref{fig:exp3_lost2} черная линия показывает прямую $x = y$, цветные линии --- графики ошибок восстановления, пунктирные --- двумя компонентами, сплошные --- одной. Чтобы ответить на поставленный вопрос, найдем точки пересечения графиков с помощью интерполяции и построим график.

        \begin{figure}[h]
            \centering
            \includegraphics[width=0.8\textwidth]{experiment_3_lost3.pdf}
            \caption{Зависимость ошибки восстановления от $\sigma^2$.}
            \label{fig:exp3_lost3}
        \end{figure}

        На рис. \ref{fig:exp3_lost3} черным пунктиром обозначена прямая $4y = x - 7$. Зависимость логарифмов угла наклона и дисперсии шума близка к уравнению $4\log_{10}(slope) = \log_{10}(\sigma^2) - 7$, поэтому сама зависимость выражается уравнением $slope^4 = 10^{-7}\sigma^2$.

    \subsection{Сравнение линейного ряда и его аппроксимации как поддерживающих рядов}
        Функции для сигналов: линейная --- $s^{(i)}_j = slope_i j+(1-slope_i\frac{N}{2})$, экспоненциальная аппроксимация --- $A \exp(slope_i j)$.
        Сигнал $\sfS^{(1)}$ --- линейная функция c наклоном $slope_1 \in \{0.0001, 0.01\}$.
        Сигналы $\sfS^{(2)}$ --- линейные функции и экспоненциальные аппроксимации c наклонами $slope_2 \in \{0.0001$, $0.0002$, $0.0005$, $0.001$, $0.002$, $0.005$, $0.01\}$. Шум первого ряда --- аддитивный гауссовский с $\sigma_1 = 0.2$. Второй ряд без шума.

        \begin{figure}[h]
            \centering
            \includegraphics[height=11cm, width=0.8\textwidth]{experiment_3_mssa2.pdf}
            \caption{Зависимость относительных ошибок от второго ряда и выбора ранга для MSSA, первый сигнал --- линейный с наклоном 0.0001.}
            \label{fig:exp3_mssa2}
        \end{figure}

        \begin{figure}[h]
            \centering
            \includegraphics[height=11cm, width=0.8\textwidth]{experiment_3_mssa1.pdf}
            \caption{Зависимость относительных ошибок от второго ряда и выбора ранга для MSSA, первый сигнал --- линейный с наклоном 0.01.}
            \label{fig:exp3_mssa1}
        \end{figure}

        На рис. \ref{fig:exp3_mssa2} $\MSSA$ хуже $\SSA$ при больших разницах наклона и наоборот для похожих сигналов. Но для восстановления двумя компонентами $\SSA$ всегда лучше. Линейная функция и ее аппроксимация поддерживают примерно одинаково.

        На рис. \ref{fig:exp3_mssa1} при использовании ранга 2 в алгоритме $\MSSA$ ошибка прогноза и восстановления меньше чем $\SSA$ почти для любого второго ряда. Использование ранга 1 может дать еще меньшую ошибку, но редко. 


    \subsection{Результат третьего эксперимента}
        Так как первая компонента разложения линейного ряда оказалась не экспонентой, это значит, что сигналы не полностью согласованы.

        Для линейных функций с большим наклоном $\MSSA$ лучше чем $\SSA$, а маленьким наклоном наоборот.
        % Использовать вторую компоненту следует только при больших углах наклона




    \conclusion
        Найдено много интересных зависимостей: линейная зависимость граничного значения среднеквадратичного отклонения и изменения параметра поддерживающего ряда, экспоненциальная зависимость доли второй компоненты в линейном ряду, степенная зависимость дисперсии шума при котором теряется вторая компонента линейного ряда от угла наклона.

        Экспоненциальную аппроксимацию линейного ряда можно использовать в качестве поддерживающего ряда для линейных рядов с большим наклоном.
        % Сделаны подготовления для изучения использования экспоненциального ряда в качестве поддерживающего для линейного и наоборот.
 
        % Какой ранг следует использовать в $\MSSA$  будет зависеть от угла наклона
        
        % \begin{itemize}
        %     \item Как по структуре рядов понять согласованность?
        %     \item Что если ранги рядов разные?
        % \end{itemize}

    
    % \section{Литература}
	% \nocite{L1}

	\renewcommand{\refname}{}
	\vspace{-25pt}
	\bibliographystyle{ugost2008}
	\bibliography{references}
\end{document}